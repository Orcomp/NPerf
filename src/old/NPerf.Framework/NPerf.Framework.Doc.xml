<?xml version="1.0" encoding="utf-8" ?> 
<doc>
<remarkss>
<remarks name="PrefTestAttribute">
<para>
The Test attribute marks a specific method inside a class that has 
already been marked with the <see cref="PerfTesterAttribute"/>, 
as a performance test method. 
</para>
<para>
The method should take the tested type as parameter and return
<c>void</c>:
<code>[PerfTester(typeof(MyClass))]
public Tester
{
    [PerfTest]
    public Test(MyClass tested)
    {
       ...
    }
}</code>
</para>
</remarks>
<remarks name="PerfTesterAttribute"> 
<para>
This is the attribute that marks a class that contains performance test
methods. This attribute is contained in the NPerf.Framework namespace. 
</para>
<para>
The user must specify the type to which the performance tests are to
be applied. This type is supplied in the constructor.
</para>
</remarks>
<remarks name="PerfTearDownAttribute">
<para>
The PerfTearDown attribute marks a specific method inside a class, that has 
already been marked with the <see cref="PerfTesterAttribute"/>, 
as a test teardown method. 
</para>
<para>
The method must take the tested type as parameter.
</para>
</remarks>
<remarks name="PerfSetUpAttribute">
<para>
The SetUp attribute marks a specific method inside a class that has 
already been marked with the <see cref="PerfTesterAttribute"/>, 
as a test setup method. 
</para>
<para>
The method takes the test index and the tested type as parameter.
The method must return void.
</para>
<para>
Because of .Net JIT, a first run must be made on the test in order to avoid the
overhead of the compilation. In this case, the testIndex is set to -1.
</para>
</remarks>
<remarks name="PerfRunDescriptorAttribute">
<para>
The <see cref="PerfRunDescriptorAttribute"/> tags a method that returns
the feature tested value in function of the run index.
</para>
<para>
For instance, if you are testing collections with increasing number of elements,
the feature tested value is the number of elements processed.
</para>
</remarks>
</remarkss>
<examples>
<example name="PerfTesterAttribute">
In this example, we mark a class to benchmark sorter algorithms defined by the <c>ISorter</c> interface,
and that should run 3 times:
<code>using NPerf.Framework;

[PerfTester(typeof(IDictionary),10)]
public class SorterTester
{
   ...
}</code>
A test description, and a featured value description can be specified in the attribute
constructor:
<code>[PerfTester(typeof(IDictionary),10, Description="...", FeatureDescription="...")]
public class SorterTester ...
</code>
</example>
<example name="PerfTestAttribute">
In this example, we define a test method. 
<code>using NPerf.Framework;

[PerfTester(typeof(IDictionary), 10)]
public DictionaryTester
{
    [PerfTest]
    public void Test(IDictionary dictionary)
    {
       ...
    }
}</code>
</example>
<example name="PerfSetUpTearDownAttribute">
The <see cref="PerfSetUpAttribute"/> defines a method that initializes the tester and
tested object. The <see cref="PerfTearDownAttribute"/> defines a method that cleans ressources after the test has 
been executed.
<code>using NPerf.Framework;

[PerfTester(typeof(IDictionary),3)]
public DictionaryTester
{
    [PerfSetUp]
    public void SetUp(int testIndex, IDictionary dic)
    {
       // setting up the test
       ...
    }
    
    [PerfTearDown]
    public void TearDown(IDictionary dic)
    {
    }
}</code>
</example>
</examples>
</doc>
